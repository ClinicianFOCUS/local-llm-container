services:
  llm-container:
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities:
                - gpu
    build:
      context: .
      dockerfile: Dockerfile.llm-cont
    user: "1000:1000" # Set the user to '1000:1000'
    volumes:
      - ./models:/models
    networks:
      - llm-network
    ipc: host
    command: --model /models/gemma-2-2b-it --rope-scaling 3
  caddy:
    container_name: caddy-llm-container # Name of the container
    user: "1000:1000" # Set the user to '1000:1000'
    build:
      context: .
      dockerfile: Dockerfile.caddy
    ports:
      - 3334:3334
    networks:
      - llm-network
    depends_on:
      - llm-container

networks:
  llm-network:
    driver: bridge
