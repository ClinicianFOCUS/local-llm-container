services:
  # Ollama service configuration - Main LLM service
  ollama:
    build:
      context: . # Build context is the current directory
      dockerfile: Dockerfile.ollama # Use the Ollama-specific Dockerfile
    user: "appuser:appgroup" # Run as user:group 1000:1000
    container_name: ollama # Set a specific name for the container
    restart: unless-stopped # Automatically restart container unless manually stopped
    volumes:
        - ./ollama-entrypoint.sh:/ollama-entrypoint.sh
    environment:
      - OLLAMA_KEEP_ALIVE=-1 # Keep the service running indefinitely
      - NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-all} # Configure GPU visibility, defaults to all
      - OLLAMA_CONCURRENT_REQUESTS=${OLLAMA_CONCURRENT_REQUESTS:-1} # Number of concurrent requests, defaults to 1
      - OLLAMA_QUEUE_ENABLED=${OLLAMA_QUEUE_ENABLED:-true} # Enable request queuing, defaults to true
    networks:
      - ollama_network # Connect to the ollama_network bridge network
    entrypoint: ["/usr/bin/bash", "/ollama-entrypoint.sh"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia # Specify NVIDIA as the GPU driver
              count: all # Use all available GPUs
              capabilities: [gpu] # Enable GPU capabilities

  # FastAPI wrapper service configuration - API interface for Ollama
  # No need for env on the port and host as it runs internal on our docker network
  fastapi-wrapper:
    build:
      context: . # Build context is the current directory
      dockerfile: Dockerfile.wrapper # Use the wrapper-specific Dockerfile
    user: "appuser:appgroup" # Run as user:group 1000:1000
    container_name: authentication-ollama # Set container name
    restart: unless-stopped # Automatic restart policy
    environment:
      - PYTHONUNBUFFERED=1 # Enable unbuffered Python output
      - SESSION_API_KEY=${SESSION_API_KEY:-} # Optional API key for authentication
    depends_on:
      - ollama # Ensure Ollama service starts first
    command: "uvicorn api_wrapper:app --host 0.0.0.0 --port 5000 --log-level debug" # Start FastAPI server
    networks:
      - ollama_network # Connect to the same network as Ollama

  # Caddy service configuration - Reverse proxy and HTTPS
  caddy:
    container_name: caddy-ollama # Container name for Caddy service
    restart: unless-stopped # Automatic restart policy
    user: "appuser:appgroup" # Run as user:group 1000:1000
    build:
      context: . # Build context is current directory
      dockerfile: Dockerfile.caddy # Use Caddy-specific Dockerfile
    environment:
      - PUBLIC_ACCESS_PORT=${PUBLIC_ACCESS_PORT:-3334} # The public access port we are using
    ports:
      # Expose the service port, configurable via PUBLIC_ACCESS_PORT env var
      - ${PUBLIC_ACCESS_PORT:-3334}:${PUBLIC_ACCESS_PORT:-3334}
    networks:
      - ollama_network # Connect to the same network as other services

# Network configuration
networks:
  ollama_network:
    driver: bridge # Create a bridge network for container communication
